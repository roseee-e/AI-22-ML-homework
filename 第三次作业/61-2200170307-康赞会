import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_curve, precision_score, recall_score, f1_score, auc

path = "C:/Users[表情]etorest/Desktop/学习/大二下/机器学习/练习/练习3/ex2data1.txt"     # 数据源路径
data = pd.read_csv(path)    # 读取数据
data.head()  # 返回data中前几行数据

# 特征向量与标签提取
cols = data.shape[1]
data = data.values
x_data = data[:, 0:cols - 1]    # 去除标签列，留下特征向量
y_data = data[:, cols - 1:cols]     # 保留标签列


# 对数据进行归一化处理
def normalize(data):
    norm = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))    # 沿行方向（垂直方向）操作
    return norm


x_data = normalize(x_data)


# 建立逻辑回归模型 ， 使用sigmoid
def sigmoid(z):
    return 1 / (1 + np.exp(-z))


# 损失函数
def computeCost(X, Y, W):
    P = sigmoid(np.dot(X, W))
    loss = np.sum(-Y * np.log(P) - (1 - Y) * np.log(1 - P)) / X.shape[0]
    return loss, P


# 计算损失函数对w参数的导数
def gradientDecent(W, X, Y):
    error = sigmoid(np.dot(X, W)) - Y
    grad = np.dot(X.T, error) / X.shape[1]
    W -= alpha * grad
    return W


# 逻辑回归参数训练过程
def logisticRegression(X, Y, iters):
    loss_his = []  # 初始化损失历史值
    W_his = []      # 初始化模型参数
    feature_dim = X.shape[1]
    W = np.zeros((feature_dim, 1))  # 初始化W系数矩阵，w 是一个(feature_dim,1)矩阵
    # 重复步骤2和步骤3，直到收敛或迭代结束
    for i in range(iters):
        #  步骤2: 使用初始化的参数来预测输出值并计算出损失值
        loss, P = computeCost(X, Y, W)
        loss_his.append(loss)
        # 步骤3: 使用梯度下降法更新参数
        W_his.append(W.copy())
        W = gradientDecent(W, X, Y)
    return loss_his, W, W_his  # 返回损失和模型参数。


alpha = 0.0001
iters = 10000

# 初始化相关数据，用以存储每次的损失值以及查全率等数据
train_loss = []
test_loss = []

W_train_his = []
precision_his = []
recall_his = []
F1_score_his = []
fpr_his = []
tpr_his = []


# 5折交叉验证，5次循环
kf = StratifiedKFold(n_splits=5, shuffle=True)

for train_index, test_index in kf.split(x_data, y_data):
    x_train, x_test = x_data[train_index], x_data[test_index]
    y_train, y_test = y_data[train_index], y_data[test_index]

    train_loss_his, W_train, W_his = logisticRegression(x_train, y_train, iters)
    W_train_his.append(W_train)

    test_loss_his = []
    for W in W_his:
        loss, _ = computeCost(x_test, y_test, W)
        test_loss_his.append(loss)

    train_loss.append(train_loss_his)
    test_loss.append(test_loss_his)

    # 测试集预测结果
    test_pred = sigmoid(np.dot(x_test, W_train))
    binary_pred = (test_pred >= 0.5).astype(int)

    precision_his.append(precision_score(y_test, binary_pred))
    recall_his.append(recall_score(y_test, binary_pred))
    F1_score_his.append(f1_score(y_test, binary_pred))
    fpr, tpr, _ = roc_curve(y_test, test_pred)
    fpr_his.append(fpr)
    tpr_his.append(tpr)

# 计算相关平均值
train_loss_avg = np.mean(train_loss, axis=0)
test_loss_avg = np.mean(test_loss, axis=0)
W_train_his_avg = np.mean(W_train_his, axis=0)

precision_avg = np.mean(precision_his)
recall_avg = np.mean(recall_his)
f1_avg = np.mean(F1_score_his)

fpr_grid = np.linspace(0.0, 1.0, 100)   # 初始化，0~1 的100数序列
mean_tpr = np.zeros_like(fpr_grid)
for i in range(5):
    mean_tpr += np.interp(fpr_grid, fpr_his[i], tpr_his[i])
fpr_grid = np.insert(fpr_grid, 0, 0.0)
mean_tpr = np.insert(mean_tpr, 0, 0.0)
mean_tpr = mean_tpr/5
roc_auc = auc(fpr_grid, mean_tpr)

# 打印所求数据
print("Precision： %f\n" % precision_avg)
print("Recall： %f\n" % recall_avg)
print("F1score： %f\n" % f1_avg)
print("Auc： %f\n" % roc_auc)

plt.plot(np.arange(iters), train_loss_avg, c='r', label='Training Loss')
plt.plot(np.arange(iters), test_loss_avg, c='b', label='Test Loss')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Training and Test Loss Curves')
plt.legend(['Train Loss', 'Test Loss'])
plt.show()

plt.plot(fpr_grid, mean_tpr, c='b', label='ROC Curve')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.legend(loc='lower right')
plt.show()
